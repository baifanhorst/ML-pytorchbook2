{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba1239cf-755e-45df-9ed0-e32b69eb51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4465eff-24bd-4a51-9966-8445ccbbac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary\n",
    "# Create dataset\n",
    "# Create a linear model and a loss function\n",
    "# Create a training loop including backpropagation and parameter update\n",
    "# Use existing optimizers in pytorch\n",
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab65939-beef-4625-a09a-ade1558c3def",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6497da6-2eed-4eb8-946f-b1aeaa4ae291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset for linear regression, y=wx+b\n",
    "# Targets (y)\n",
    "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\n",
    "# Inputs (x)\n",
    "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(2, 2)) # Width 2 inches, height 2 inches\n",
    "ax.scatter(t_u, t_c)\n",
    "\n",
    "# Rescaling the input for better gradient computation\n",
    "# Some optimizers are more sensitive to the scale of the data, e.g. the stochastic gradient descent\n",
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26fa58-afdf-441d-a2d6-312e64413860",
   "metadata": {},
   "source": [
    "# Create a linear model and a loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a348798f-70cc-4325-8a2b-77d4eec0f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    \"\"\"\n",
    "    t_u: input tensor, currently 1D\n",
    "    w: weight parameter, 0D tensor\n",
    "    b: bias, 0D tensor\n",
    "    \"\"\"\n",
    "    return w * t_u + b\n",
    "\n",
    "def loss_fn(t_p, t_c):\n",
    "    \"\"\"\n",
    "    t_p: predicted values\n",
    "    t_c: true values\n",
    "    \"\"\"\n",
    "    # Return the mean of the squared difference between predictions and targets\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a9b82a6-d806-4501-b983-848614bb07c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4517.2969,   82.6000])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "## Test the model and the loss function\n",
    "\n",
    "# Initialize weight and bias as the two elements of a single tensor\n",
    "# This will be convenient when we do the gradient descent, \n",
    "    # where we only need a single line to update both parameters\n",
    "# \"requires_grad=True\" causes all tensors resulted from 'params' \n",
    "    # to create computation graph to compute gradient\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "\n",
    "\n",
    "# Compute the loss \n",
    "loss = loss_fn(model(t_u, *params), t_c)\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "# Show the gradient of 'loss' wrt the 'params'.\n",
    "# Note that the gradient is stored at 'params', not 'loss'\n",
    "print(params.grad)\n",
    "\n",
    "# Setting the gradient to be zero for further gradient calculation\n",
    "# For example, if there is another tensor T created from 'params' \n",
    "    # and we compute its gradient by T.backward(),\n",
    "    # then the gradient will be added (accumulated) to the existing gradient wrt 'loss'\n",
    "# Thus, it is a common practice to reset gradient to zero before each back propagation\n",
    "if params.grad is not None:\n",
    "    params.grad.zero_()\n",
    "    print(params.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784957e-c71c-4703-8158-ed5d9c5f3fec",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69ea2772-d79d-4c59-9cce-bee5f1ade208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860115051269531\n",
      "Epoch 1000, Loss 3.828537940979004\n",
      "Epoch 1500, Loss 3.092191219329834\n",
      "Epoch 2000, Loss 2.957697868347168\n",
      "Epoch 2500, Loss 2.933133840560913\n",
      "Epoch 3000, Loss 2.9286484718322754\n",
      "Epoch 3500, Loss 2.9278297424316406\n",
      "Epoch 4000, Loss 2.9276793003082275\n",
      "Epoch 4500, Loss 2.927651882171631\n",
      "Epoch 5000, Loss 2.9276468753814697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    \"\"\"\n",
    "    n_epochs: number of epochs\n",
    "    learning_rate: learning rate\n",
    "    params: parameters\n",
    "    t_u: input (x in the dataset)\n",
    "    t_c: targets (y in the dataset)\n",
    "    \"\"\"\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fn(model(t_u, *params), t_c)\n",
    "\n",
    "        # Reset gradient to zero\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        # We simply change the parameter values, not create a new tensor\n",
    "        # We can check that params.grad.requires_grad is False.\n",
    "        # Without the 'torch.no_grad()' context, 'params -= learning_rate * params.grad' would raise an error\n",
    "        with torch.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "        # Show the loss during training\n",
    "        if epoch % 500 == 0:\n",
    "            print(f'Epoch {epoch}, Loss {float(loss)}')\n",
    "    return params\n",
    "\n",
    "# Note that the rescaled input (t_un) is used\n",
    "training_loop(n_epochs = 5000, learning_rate = 1e-2, \n",
    "              params = torch.tensor([1.0, 0.0], requires_grad=True), t_u = t_un, t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a4371-3a2a-44e7-ad6d-50228e01975e",
   "metadata": {},
   "source": [
    "# Use existing optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2365a8f9-61cc-42b5-b202-d4b1ccde3175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASGD', 'Adadelta', 'Adafactor', 'Adagrad', 'Adam', 'AdamW', 'Adamax', 'LBFGS', 'NAdam', 'Optimizer', 'RAdam', 'RMSprop', 'Rprop', 'SGD', 'SparseAdam', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_adafactor', '_functional', 'lr_scheduler', 'swa_utils']\n"
     ]
    }
   ],
   "source": [
    "# Show the optimization alg in the torch.optim module\n",
    "print( dir(optim) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27954ef4-bf0b-4529-a186-da1050fb76d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# stochastic gradient descent\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "# Note that 'params' must be put in a list (in general, this argument should be an iterable)\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "## Test the optimizer\n",
    "# Evaluate loss\n",
    "loss = loss_fn(model(t_u, *params), t_c)\n",
    "# Compute derivatives\n",
    "# Before each backpropagation, reset gradient to zero\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "# Update parameters\n",
    "# This is the same as manually updating the parameters\n",
    "# by substracting learning_rate * gradient\n",
    "optimizer.step()\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aef51d18-8f92-4ee3-96cf-788087049bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860115051269531\n",
      "Epoch 1000, Loss 3.828537940979004\n",
      "Epoch 1500, Loss 3.092191219329834\n",
      "Epoch 2000, Loss 2.957697868347168\n",
      "Epoch 2500, Loss 2.933133840560913\n",
      "Epoch 3000, Loss 2.9286484718322754\n",
      "Epoch 3500, Loss 2.9278297424316406\n",
      "Epoch 4000, Loss 2.9276793003082275\n",
      "Epoch 4500, Loss 2.927651882171631\n",
      "Epoch 5000, Loss 2.9276468753814697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training with SGD\n",
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        loss = loss_fn(model(t_u, *params), t_c)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(f'Epoch {epoch}, Loss {float(loss)}')\n",
    "    return params\n",
    "\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "optimizer = optim.SGD([params], lr=1e-2)\n",
    "n_epochs = 5000\n",
    "training_loop(n_epochs, optimizer, params, t_un, t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "46b39992-1b47-4154-a560-3cd80f4b1b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.612898349761963\n",
      "Epoch 1000, Loss 3.086700439453125\n",
      "Epoch 1500, Loss 2.928579092025757\n",
      "Epoch 2000, Loss 2.9276442527770996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.5367, -17.3021], requires_grad=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the Adam optimizer\n",
    "# This optimizer is less sensitive to the scale difference between parameters\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "# Use a larger learning rate\n",
    "learning_rate = 1e-1\n",
    "optimizer = optim.Adam([params], lr=learning_rate)\n",
    "# Use less epochs\n",
    "n_epochs = 2000\n",
    "# Note that t_u is used instead of t_un\n",
    "training_loop(n_epochs, optimizer, params, t_u, t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688cd024-3327-4107-96b6-888084b8a854",
   "metadata": {},
   "source": [
    "# Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a90708e-d60f-42bc-958d-78d0f7b755a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples 11\n",
      "Validation set size 2\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset\n",
    "n_samples = t_u.shape[0]\n",
    "print(\"Number of samples\", n_samples)\n",
    "n_val = int(0.2 * n_samples)\n",
    "print(\"Validation set size\", n_val)\n",
    "# Shuffling the indices of the dataset. The result is a tensor\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "# Indices of training and validating sets\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "# Training set\n",
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "# Validation set\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "# Rescaled training set\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3328ed5a-c6d5-4189-b44f-157151c234b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 98.0567, Validation loss 0.7488\n",
      "Epoch 2, Training loss 37.8207, Validation loss 13.0239\n",
      "Epoch 3, Training loss 30.7609, Validation loss 24.6283\n",
      "Epoch 500, Training loss 7.5900, Validation loss 9.0263\n",
      "Epoch 1000, Training loss 3.2741, Validation loss 5.7568\n",
      "Epoch 1500, Training loss 2.4474, Validation loss 5.6629\n",
      "Epoch 2000, Training loss 2.2891, Validation loss 5.8779\n",
      "Epoch 2500, Training loss 2.2588, Validation loss 6.0210\n",
      "Epoch 3000, Training loss 2.2530, Validation loss 6.0931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.4576, -17.7181], requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the training loop\n",
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,\n",
    "                    train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Forward propagation (computing the loss)\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "\n",
    "        # Evaluate the loss on the validation set.\n",
    "        # Since no computation graph is required, we turn off autograd\n",
    "        with torch.no_grad():\n",
    "            val_t_p = model(val_t_u, *params)\n",
    "            val_loss = loss_fn(val_t_p, val_t_c)\n",
    "            # Within this context, val_loss.requires_grad should be False\n",
    "            # assert condition raises an error if the condition is not satisfied\n",
    "            assert val_loss.requires_grad == False\n",
    "\n",
    "        # Back propagation\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch <= 3 or epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                    f\" Validation loss {val_loss.item():.4f}\")\n",
    "    return params\n",
    "\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "training_loop(n_epochs = 3000, optimizer = optimizer, params = params, \n",
    "                train_t_u = train_t_un, val_t_u = val_t_un,\n",
    "                train_t_c = train_t_c, val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0aa3f6aa-58a7-4ec7-baf4-21dc3897d2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 98.0567, Validation loss 0.7488\n",
      "Epoch 2, Training loss 37.8207, Validation loss 13.0239\n",
      "Epoch 3, Training loss 30.7609, Validation loss 24.6283\n",
      "Epoch 500, Training loss 7.5900, Validation loss 9.0263\n",
      "Epoch 1000, Training loss 3.2741, Validation loss 5.7568\n",
      "Epoch 1500, Training loss 2.4474, Validation loss 5.6629\n",
      "Epoch 2000, Training loss 2.2891, Validation loss 5.8779\n",
      "Epoch 2500, Training loss 2.2588, Validation loss 6.0210\n",
      "Epoch 3000, Training loss 2.2530, Validation loss 6.0931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.4576, -17.7181], requires_grad=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way to turn on/off autograd\n",
    "def calc_forward(t_u, t_c, is_train):\n",
    "    # If is_train==False, turn on autograd, otherwise turn it off\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        loss = loss_fn(model(t_u, *params), t_c)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Update the training loop\n",
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,\n",
    "                    train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Forward propagation (computing the loss)\n",
    "        train_loss = calc_forward(train_t_u, train_t_c, True)\n",
    "        val_loss = calc_forward(val_t_u, val_t_c, False)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch <= 3 or epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                    f\" Validation loss {val_loss.item():.4f}\")\n",
    "    return params\n",
    "\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "training_loop(n_epochs = 3000, optimizer = optimizer, params = params, \n",
    "                train_t_u = train_t_un, val_t_u = val_t_un,\n",
    "                train_t_c = train_t_c, val_t_c = val_t_c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
